{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6f76974",
   "metadata": {},
   "source": [
    "<h1><b>Data Preprocessing & Cleaning</h1></b>\n",
    "\n",
    "<h2>Contents</h2>\n",
    "\n",
    "1. Data Cleaning\n",
    "2. Lemmatization\n",
    "3. Stemming\n",
    "4. Tokenization\n",
    "\n",
    "Source Code:\n",
    "https://techcommunity.microsoft.com/t5/educator-developer-blog/how-to-scrape-twitter-data-for-sentiment-analysis-with-python/ba-p/3593365\n",
    "\n",
    "To conduct data cleaning ONLY, click [here](#Section1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "203ccb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download nltk packages\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ee4fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afc15791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import contractions\n",
    "\n",
    "#to detect text language\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "#stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#Lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "029c7c24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>ID</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-30 23:41:11+00:00</td>\n",
       "      <td>1641586401568509952</td>\n",
       "      <td>NumanAfifi</td>\n",
       "      <td>@ashvinmenon Turns out theyâ€™re the best when t...</td>\n",
       "      <td>Kuala Lumpur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-30 12:07:36+00:00</td>\n",
       "      <td>1641411855225851905</td>\n",
       "      <td>marcosrahman</td>\n",
       "      <td>this week is super hectic. literally the harde...</td>\n",
       "      <td>Kuala Lumpur, Malaysia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-30 10:12:34+00:00</td>\n",
       "      <td>1641382904185110530</td>\n",
       "      <td>LynnTweetss</td>\n",
       "      <td>Hungry or annoyed or just grumpy?</td>\n",
       "      <td>Kuala Lumpur City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-30 06:00:53+00:00</td>\n",
       "      <td>1641319565018959872</td>\n",
       "      <td>lzmnilz</td>\n",
       "      <td>sukiya!! tomorrow!! cepat!! hungry!!</td>\n",
       "      <td>Kuala Lumpur City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-30 04:00:54+00:00</td>\n",
       "      <td>1641289370824232960</td>\n",
       "      <td>Hemarubeny30</td>\n",
       "      <td>So hungry ðŸ˜©</td>\n",
       "      <td>kajang,Malaysia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>2022-03-06 04:25:51+00:00</td>\n",
       "      <td>1500326770817773569</td>\n",
       "      <td>ffarhan26</td>\n",
       "      <td>@zulhilmi_imi Ah to ease your hunger\\nâœ¨at leas...</td>\n",
       "      <td>Malaysia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>2022-02-26 15:19:57+00:00</td>\n",
       "      <td>1497592277778452481</td>\n",
       "      <td>sarahshdn</td>\n",
       "      <td>@cygaraga I may add some since I've been there...</td>\n",
       "      <td>Kuala Lumpur, Malaysia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>2022-02-10 14:08:20+00:00</td>\n",
       "      <td>1491776048253390848</td>\n",
       "      <td>al_seng</td>\n",
       "      <td>@AzalinaOthmanS @MYParlimen To me, for a count...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>2022-02-09 08:00:03+00:00</td>\n",
       "      <td>1491320979880288256</td>\n",
       "      <td>afmnurdin</td>\n",
       "      <td>dia mcm hunger game or maze runner show dah. S...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>2022-02-06 23:41:36+00:00</td>\n",
       "      <td>1490470763723251713</td>\n",
       "      <td>7partsofsoul</td>\n",
       "      <td>Nak baca balik la The Hunger Games.</td>\n",
       "      <td>Damansara</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>965 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Datetime                   ID      Username   \n",
       "0    2023-03-30 23:41:11+00:00  1641586401568509952    NumanAfifi  \\\n",
       "1    2023-03-30 12:07:36+00:00  1641411855225851905  marcosrahman   \n",
       "2    2023-03-30 10:12:34+00:00  1641382904185110530   LynnTweetss   \n",
       "3    2023-03-30 06:00:53+00:00  1641319565018959872       lzmnilz   \n",
       "4    2023-03-30 04:00:54+00:00  1641289370824232960  Hemarubeny30   \n",
       "..                         ...                  ...           ...   \n",
       "960  2022-03-06 04:25:51+00:00  1500326770817773569     ffarhan26   \n",
       "961  2022-02-26 15:19:57+00:00  1497592277778452481     sarahshdn   \n",
       "962  2022-02-10 14:08:20+00:00  1491776048253390848       al_seng   \n",
       "963  2022-02-09 08:00:03+00:00  1491320979880288256     afmnurdin   \n",
       "964  2022-02-06 23:41:36+00:00  1490470763723251713  7partsofsoul   \n",
       "\n",
       "                                                  Text                Location  \n",
       "0    @ashvinmenon Turns out theyâ€™re the best when t...            Kuala Lumpur  \n",
       "1    this week is super hectic. literally the harde...  Kuala Lumpur, Malaysia  \n",
       "2                    Hungry or annoyed or just grumpy?       Kuala Lumpur City  \n",
       "3                 sukiya!! tomorrow!! cepat!! hungry!!       Kuala Lumpur City  \n",
       "4                                          So hungry ðŸ˜©         kajang,Malaysia  \n",
       "..                                                 ...                     ...  \n",
       "960  @zulhilmi_imi Ah to ease your hunger\\nâœ¨at leas...                Malaysia  \n",
       "961  @cygaraga I may add some since I've been there...  Kuala Lumpur, Malaysia  \n",
       "962  @AzalinaOthmanS @MYParlimen To me, for a count...                     NaN  \n",
       "963  dia mcm hunger game or maze runner show dah. S...                     NaN  \n",
       "964                Nak baca balik la The Hunger Games.              Damansara   \n",
       "\n",
       "[965 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of original rows: 965\n"
     ]
    }
   ],
   "source": [
    "# Load csv file containing tweets dataset\n",
    "def load_data():\n",
    "    data = pd.read_csv(r\"C:\\Users\\LENOVO\\Documents\\Degree Life\\FYP Journey\\Dataset\\Python Scraping\\Snscrape\\Feb 2022 - Mar 2023\\Hungry OR Hunger\\States\\Hungry OR Hunger_Kuala Lumpur [Coordinates] - Snscrape.csv\")\n",
    "    return data\n",
    "\n",
    "tweet_df = load_data()\n",
    "display(tweet_df)\n",
    "\n",
    "print('Number of original rows:', len(tweet_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09df3388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-30 23:41:11+00:00</td>\n",
       "      <td>NumanAfifi</td>\n",
       "      <td>@ashvinmenon Turns out theyâ€™re the best when t...</td>\n",
       "      <td>Kuala Lumpur</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-30 12:07:36+00:00</td>\n",
       "      <td>marcosrahman</td>\n",
       "      <td>this week is super hectic. literally the harde...</td>\n",
       "      <td>Kuala Lumpur, Malaysia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-30 10:12:34+00:00</td>\n",
       "      <td>LynnTweetss</td>\n",
       "      <td>Hungry or annoyed or just grumpy?</td>\n",
       "      <td>Kuala Lumpur City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-30 06:00:53+00:00</td>\n",
       "      <td>lzmnilz</td>\n",
       "      <td>sukiya!! tomorrow!! cepat!! hungry!!</td>\n",
       "      <td>Kuala Lumpur City</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-30 04:00:54+00:00</td>\n",
       "      <td>Hemarubeny30</td>\n",
       "      <td>So hungry ðŸ˜©</td>\n",
       "      <td>kajang,Malaysia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>2022-03-06 04:25:51+00:00</td>\n",
       "      <td>ffarhan26</td>\n",
       "      <td>@zulhilmi_imi Ah to ease your hunger\\nâœ¨at leas...</td>\n",
       "      <td>Malaysia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>2022-02-26 15:19:57+00:00</td>\n",
       "      <td>sarahshdn</td>\n",
       "      <td>@cygaraga I may add some since I've been there...</td>\n",
       "      <td>Kuala Lumpur, Malaysia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>2022-02-10 14:08:20+00:00</td>\n",
       "      <td>al_seng</td>\n",
       "      <td>@AzalinaOthmanS @MYParlimen To me, for a count...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>2022-02-09 08:00:03+00:00</td>\n",
       "      <td>afmnurdin</td>\n",
       "      <td>dia mcm hunger game or maze runner show dah. S...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>2022-02-06 23:41:36+00:00</td>\n",
       "      <td>7partsofsoul</td>\n",
       "      <td>Nak baca balik la The Hunger Games.</td>\n",
       "      <td>Damansara</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>965 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Datetime      Username   \n",
       "0    2023-03-30 23:41:11+00:00    NumanAfifi  \\\n",
       "1    2023-03-30 12:07:36+00:00  marcosrahman   \n",
       "2    2023-03-30 10:12:34+00:00   LynnTweetss   \n",
       "3    2023-03-30 06:00:53+00:00       lzmnilz   \n",
       "4    2023-03-30 04:00:54+00:00  Hemarubeny30   \n",
       "..                         ...           ...   \n",
       "960  2022-03-06 04:25:51+00:00     ffarhan26   \n",
       "961  2022-02-26 15:19:57+00:00     sarahshdn   \n",
       "962  2022-02-10 14:08:20+00:00       al_seng   \n",
       "963  2022-02-09 08:00:03+00:00     afmnurdin   \n",
       "964  2022-02-06 23:41:36+00:00  7partsofsoul   \n",
       "\n",
       "                                                  Text                Location  \n",
       "0    @ashvinmenon Turns out theyâ€™re the best when t...            Kuala Lumpur  \n",
       "1    this week is super hectic. literally the harde...  Kuala Lumpur, Malaysia  \n",
       "2                    Hungry or annoyed or just grumpy?       Kuala Lumpur City  \n",
       "3                 sukiya!! tomorrow!! cepat!! hungry!!       Kuala Lumpur City  \n",
       "4                                          So hungry ðŸ˜©         kajang,Malaysia  \n",
       "..                                                 ...                     ...  \n",
       "960  @zulhilmi_imi Ah to ease your hunger\\nâœ¨at leas...                Malaysia  \n",
       "961  @cygaraga I may add some since I've been there...  Kuala Lumpur, Malaysia  \n",
       "962  @AzalinaOthmanS @MYParlimen To me, for a count...                     NaN  \n",
       "963  dia mcm hunger game or maze runner show dah. S...                     NaN  \n",
       "964                Nak baca balik la The Hunger Games.              Damansara   \n",
       "\n",
       "[965 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Drop unnecessary columns (ID)\n",
    "new_df = tweet_df.drop(['ID'], axis=1)\n",
    "display(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb43b5c",
   "metadata": {},
   "source": [
    "<h1><b>Data Preprocessing</h1></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2539615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d923ce7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    # Expand contractions\n",
    "    expanded_text = contractions.fix(text)\n",
    "    return expanded_text\n",
    "\n",
    "#clean the tweets with a function\n",
    "def cleanedTweets(text):\n",
    "    # Expand contractions\n",
    "    text = expand_contractions(text)\n",
    "    #lowercase text\n",
    "    text = text.lower()\n",
    "    # remove @ and links\n",
    "    text = ' '.join(re.sub(\"(@[A-Za-z0-9_]+|[^0-9A-Za-z \\t]) | (\\w+:\\/\\/\\S+)\",\" \",text).split())\n",
    "    #Remove repeating words\n",
    "    text = re.sub(r'\\@\\w+|\\#\\w+|\\d+','', text)\n",
    "    #Remove punctuation & numbers\n",
    "    punct = str.maketrans('','',string.punctuation)\n",
    "    text = text.translate(punct)\n",
    "    #Removing all URLS (hhtps and www)\n",
    "    text = re.sub(r'http\\S+','', text)\n",
    "    text = re.sub(r'www\\S+','', text)\n",
    "    # remove special characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
    "    #Tokenizing words & removing stop words from the tweets\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_words = [w for w in tokens if w not in stopwords]\n",
    "    #Lemmatizing words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "    text = \" \".join(lemma_words)\n",
    "    #Stemming words\n",
    "    stemmer = PorterStemmer()\n",
    "    stem_words = [stemmer.stem(w) for w in lemma_words]\n",
    "    \n",
    "    #Return tweets based on different phases\n",
    "    return {'text': text, 'tokens': tokens, \n",
    "            'filtered_words': filtered_words, \n",
    "            'lemma_words': lemma_words, \n",
    "            'stem_words': stem_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47f24d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "      <th>Location</th>\n",
       "      <th>Cleaned_Tweets</th>\n",
       "      <th>Cleaned_Text</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Filtered_Words</th>\n",
       "      <th>Lemmatized_Words</th>\n",
       "      <th>Stemmed_Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-30 23:41:11+00:00</td>\n",
       "      <td>NumanAfifi</td>\n",
       "      <td>@ashvinmenon Turns out theyâ€™re the best when t...</td>\n",
       "      <td>Kuala Lumpur</td>\n",
       "      <td>{'text': 'turn best hungry', 'tokens': ['turns...</td>\n",
       "      <td>turn best hungry</td>\n",
       "      <td>[turns, out, they, are, the, best, when, they,...</td>\n",
       "      <td>[turns, best, hungry]</td>\n",
       "      <td>[turn, best, hungry]</td>\n",
       "      <td>[turn, best, hungri]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-30 12:07:36+00:00</td>\n",
       "      <td>marcosrahman</td>\n",
       "      <td>this week is super hectic. literally the harde...</td>\n",
       "      <td>Kuala Lumpur, Malaysia</td>\n",
       "      <td>{'text': 'week super hectic literally hardest ...</td>\n",
       "      <td>week super hectic literally hardest week work ...</td>\n",
       "      <td>[this, week, is, super, hectic, literally, the...</td>\n",
       "      <td>[week, super, hectic, literally, hardest, week...</td>\n",
       "      <td>[week, super, hectic, literally, hardest, week...</td>\n",
       "      <td>[week, super, hectic, liter, hardest, week, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-30 10:12:34+00:00</td>\n",
       "      <td>LynnTweetss</td>\n",
       "      <td>Hungry or annoyed or just grumpy?</td>\n",
       "      <td>Kuala Lumpur City</td>\n",
       "      <td>{'text': 'hungry annoyed grumpy', 'tokens': ['...</td>\n",
       "      <td>hungry annoyed grumpy</td>\n",
       "      <td>[hungry, or, annoyed, or, just, grumpy]</td>\n",
       "      <td>[hungry, annoyed, grumpy]</td>\n",
       "      <td>[hungry, annoyed, grumpy]</td>\n",
       "      <td>[hungri, annoy, grumpi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-30 06:00:53+00:00</td>\n",
       "      <td>lzmnilz</td>\n",
       "      <td>sukiya!! tomorrow!! cepat!! hungry!!</td>\n",
       "      <td>Kuala Lumpur City</td>\n",
       "      <td>{'text': 'sukiya tomorrow cepat hungry', 'toke...</td>\n",
       "      <td>sukiya tomorrow cepat hungry</td>\n",
       "      <td>[sukiya, tomorrow, cepat, hungry]</td>\n",
       "      <td>[sukiya, tomorrow, cepat, hungry]</td>\n",
       "      <td>[sukiya, tomorrow, cepat, hungry]</td>\n",
       "      <td>[sukiya, tomorrow, cepat, hungri]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-30 04:00:54+00:00</td>\n",
       "      <td>Hemarubeny30</td>\n",
       "      <td>So hungry ðŸ˜©</td>\n",
       "      <td>kajang,Malaysia</td>\n",
       "      <td>{'text': 'hungry', 'tokens': ['so', 'hungry'],...</td>\n",
       "      <td>hungry</td>\n",
       "      <td>[so, hungry]</td>\n",
       "      <td>[hungry]</td>\n",
       "      <td>[hungry]</td>\n",
       "      <td>[hungri]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>2022-03-06 04:25:51+00:00</td>\n",
       "      <td>ffarhan26</td>\n",
       "      <td>@zulhilmi_imi Ah to ease your hunger\\nâœ¨at leas...</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>{'text': 'ah ease hunger least', 'tokens': ['a...</td>\n",
       "      <td>ah ease hunger least</td>\n",
       "      <td>[ah, to, ease, your, hunger, at, least]</td>\n",
       "      <td>[ah, ease, hunger, least]</td>\n",
       "      <td>[ah, ease, hunger, least]</td>\n",
       "      <td>[ah, eas, hunger, least]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>2022-02-26 15:19:57+00:00</td>\n",
       "      <td>sarahshdn</td>\n",
       "      <td>@cygaraga I may add some since I've been there...</td>\n",
       "      <td>Kuala Lumpur, Malaysia</td>\n",
       "      <td>{'text': 'may add since got kicked track covid...</td>\n",
       "      <td>may add since got kicked track covid change ma...</td>\n",
       "      <td>[i, may, add, some, since, i, have, been, ther...</td>\n",
       "      <td>[may, add, since, got, kicked, track, covid, c...</td>\n",
       "      <td>[may, add, since, got, kicked, track, covid, c...</td>\n",
       "      <td>[may, add, sinc, got, kick, track, covid, chan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>2022-02-10 14:08:20+00:00</td>\n",
       "      <td>al_seng</td>\n",
       "      <td>@AzalinaOthmanS @MYParlimen To me, for a count...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'text': 'country country embrace democracy la...</td>\n",
       "      <td>country country embrace democracy law guide po...</td>\n",
       "      <td>[to, me, for, a, country, a, country, which, e...</td>\n",
       "      <td>[country, country, embrace, democracy, law, gu...</td>\n",
       "      <td>[country, country, embrace, democracy, law, gu...</td>\n",
       "      <td>[countri, countri, embrac, democraci, law, gui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>2022-02-09 08:00:03+00:00</td>\n",
       "      <td>afmnurdin</td>\n",
       "      <td>dia mcm hunger game or maze runner show dah. S...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'text': 'dia mcm hunger game maze runner show...</td>\n",
       "      <td>dia mcm hunger game maze runner show dah siapa...</td>\n",
       "      <td>[dia, mcm, hunger, game, or, maze, runner, sho...</td>\n",
       "      <td>[dia, mcm, hunger, game, maze, runner, show, d...</td>\n",
       "      <td>[dia, mcm, hunger, game, maze, runner, show, d...</td>\n",
       "      <td>[dia, mcm, hunger, game, maze, runner, show, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>2022-02-06 23:41:36+00:00</td>\n",
       "      <td>7partsofsoul</td>\n",
       "      <td>Nak baca balik la The Hunger Games.</td>\n",
       "      <td>Damansara</td>\n",
       "      <td>{'text': 'nak baca balik la hunger game', 'tok...</td>\n",
       "      <td>nak baca balik la hunger game</td>\n",
       "      <td>[nak, baca, balik, la, the, hunger, games]</td>\n",
       "      <td>[nak, baca, balik, la, hunger, games]</td>\n",
       "      <td>[nak, baca, balik, la, hunger, game]</td>\n",
       "      <td>[nak, baca, balik, la, hunger, game]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>965 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Datetime      Username   \n",
       "0    2023-03-30 23:41:11+00:00    NumanAfifi  \\\n",
       "1    2023-03-30 12:07:36+00:00  marcosrahman   \n",
       "2    2023-03-30 10:12:34+00:00   LynnTweetss   \n",
       "3    2023-03-30 06:00:53+00:00       lzmnilz   \n",
       "4    2023-03-30 04:00:54+00:00  Hemarubeny30   \n",
       "..                         ...           ...   \n",
       "960  2022-03-06 04:25:51+00:00     ffarhan26   \n",
       "961  2022-02-26 15:19:57+00:00     sarahshdn   \n",
       "962  2022-02-10 14:08:20+00:00       al_seng   \n",
       "963  2022-02-09 08:00:03+00:00     afmnurdin   \n",
       "964  2022-02-06 23:41:36+00:00  7partsofsoul   \n",
       "\n",
       "                                                  Text   \n",
       "0    @ashvinmenon Turns out theyâ€™re the best when t...  \\\n",
       "1    this week is super hectic. literally the harde...   \n",
       "2                    Hungry or annoyed or just grumpy?   \n",
       "3                 sukiya!! tomorrow!! cepat!! hungry!!   \n",
       "4                                          So hungry ðŸ˜©   \n",
       "..                                                 ...   \n",
       "960  @zulhilmi_imi Ah to ease your hunger\\nâœ¨at leas...   \n",
       "961  @cygaraga I may add some since I've been there...   \n",
       "962  @AzalinaOthmanS @MYParlimen To me, for a count...   \n",
       "963  dia mcm hunger game or maze runner show dah. S...   \n",
       "964                Nak baca balik la The Hunger Games.   \n",
       "\n",
       "                   Location   \n",
       "0              Kuala Lumpur  \\\n",
       "1    Kuala Lumpur, Malaysia   \n",
       "2         Kuala Lumpur City   \n",
       "3         Kuala Lumpur City   \n",
       "4           kajang,Malaysia   \n",
       "..                      ...   \n",
       "960                Malaysia   \n",
       "961  Kuala Lumpur, Malaysia   \n",
       "962                     NaN   \n",
       "963                     NaN   \n",
       "964              Damansara    \n",
       "\n",
       "                                        Cleaned_Tweets   \n",
       "0    {'text': 'turn best hungry', 'tokens': ['turns...  \\\n",
       "1    {'text': 'week super hectic literally hardest ...   \n",
       "2    {'text': 'hungry annoyed grumpy', 'tokens': ['...   \n",
       "3    {'text': 'sukiya tomorrow cepat hungry', 'toke...   \n",
       "4    {'text': 'hungry', 'tokens': ['so', 'hungry'],...   \n",
       "..                                                 ...   \n",
       "960  {'text': 'ah ease hunger least', 'tokens': ['a...   \n",
       "961  {'text': 'may add since got kicked track covid...   \n",
       "962  {'text': 'country country embrace democracy la...   \n",
       "963  {'text': 'dia mcm hunger game maze runner show...   \n",
       "964  {'text': 'nak baca balik la hunger game', 'tok...   \n",
       "\n",
       "                                          Cleaned_Text   \n",
       "0                                     turn best hungry  \\\n",
       "1    week super hectic literally hardest week work ...   \n",
       "2                                hungry annoyed grumpy   \n",
       "3                         sukiya tomorrow cepat hungry   \n",
       "4                                               hungry   \n",
       "..                                                 ...   \n",
       "960                               ah ease hunger least   \n",
       "961  may add since got kicked track covid change ma...   \n",
       "962  country country embrace democracy law guide po...   \n",
       "963  dia mcm hunger game maze runner show dah siapa...   \n",
       "964                      nak baca balik la hunger game   \n",
       "\n",
       "                                                Tokens   \n",
       "0    [turns, out, they, are, the, best, when, they,...  \\\n",
       "1    [this, week, is, super, hectic, literally, the...   \n",
       "2              [hungry, or, annoyed, or, just, grumpy]   \n",
       "3                    [sukiya, tomorrow, cepat, hungry]   \n",
       "4                                         [so, hungry]   \n",
       "..                                                 ...   \n",
       "960            [ah, to, ease, your, hunger, at, least]   \n",
       "961  [i, may, add, some, since, i, have, been, ther...   \n",
       "962  [to, me, for, a, country, a, country, which, e...   \n",
       "963  [dia, mcm, hunger, game, or, maze, runner, sho...   \n",
       "964         [nak, baca, balik, la, the, hunger, games]   \n",
       "\n",
       "                                        Filtered_Words   \n",
       "0                                [turns, best, hungry]  \\\n",
       "1    [week, super, hectic, literally, hardest, week...   \n",
       "2                            [hungry, annoyed, grumpy]   \n",
       "3                    [sukiya, tomorrow, cepat, hungry]   \n",
       "4                                             [hungry]   \n",
       "..                                                 ...   \n",
       "960                          [ah, ease, hunger, least]   \n",
       "961  [may, add, since, got, kicked, track, covid, c...   \n",
       "962  [country, country, embrace, democracy, law, gu...   \n",
       "963  [dia, mcm, hunger, game, maze, runner, show, d...   \n",
       "964              [nak, baca, balik, la, hunger, games]   \n",
       "\n",
       "                                      Lemmatized_Words   \n",
       "0                                 [turn, best, hungry]  \\\n",
       "1    [week, super, hectic, literally, hardest, week...   \n",
       "2                            [hungry, annoyed, grumpy]   \n",
       "3                    [sukiya, tomorrow, cepat, hungry]   \n",
       "4                                             [hungry]   \n",
       "..                                                 ...   \n",
       "960                          [ah, ease, hunger, least]   \n",
       "961  [may, add, since, got, kicked, track, covid, c...   \n",
       "962  [country, country, embrace, democracy, law, gu...   \n",
       "963  [dia, mcm, hunger, game, maze, runner, show, d...   \n",
       "964               [nak, baca, balik, la, hunger, game]   \n",
       "\n",
       "                                         Stemmed_Words  \n",
       "0                                 [turn, best, hungri]  \n",
       "1    [week, super, hectic, liter, hardest, week, wo...  \n",
       "2                              [hungri, annoy, grumpi]  \n",
       "3                    [sukiya, tomorrow, cepat, hungri]  \n",
       "4                                             [hungri]  \n",
       "..                                                 ...  \n",
       "960                           [ah, eas, hunger, least]  \n",
       "961  [may, add, sinc, got, kick, track, covid, chan...  \n",
       "962  [countri, countri, embrac, democraci, law, gui...  \n",
       "963  [dia, mcm, hunger, game, maze, runner, show, d...  \n",
       "964               [nak, baca, balik, la, hunger, game]  \n",
       "\n",
       "[965 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply the cleanedTweets function to the 'Text' column and create new columns for each phase\n",
    "new_df[['Cleaned_Text', 'Tokens', 'Filtered_Words', 'Lemmatized_Words', 'Stemmed_Words']] = new_df['Text'].apply(cleanedTweets).apply(pd.Series)\n",
    "display(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2026d37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Username</th>\n",
       "      <th>Text</th>\n",
       "      <th>Location</th>\n",
       "      <th>Cleaned_Tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-03-30 23:41:11+00:00</td>\n",
       "      <td>NumanAfifi</td>\n",
       "      <td>@ashvinmenon Turns out theyâ€™re the best when t...</td>\n",
       "      <td>Kuala Lumpur</td>\n",
       "      <td>{'text': 'turn best hungry', 'tokens': ['turns...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-03-30 12:07:36+00:00</td>\n",
       "      <td>marcosrahman</td>\n",
       "      <td>this week is super hectic. literally the harde...</td>\n",
       "      <td>Kuala Lumpur, Malaysia</td>\n",
       "      <td>{'text': 'week super hectic literally hardest ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-03-30 10:12:34+00:00</td>\n",
       "      <td>LynnTweetss</td>\n",
       "      <td>Hungry or annoyed or just grumpy?</td>\n",
       "      <td>Kuala Lumpur City</td>\n",
       "      <td>{'text': 'hungry annoyed grumpy', 'tokens': ['...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-03-30 06:00:53+00:00</td>\n",
       "      <td>lzmnilz</td>\n",
       "      <td>sukiya!! tomorrow!! cepat!! hungry!!</td>\n",
       "      <td>Kuala Lumpur City</td>\n",
       "      <td>{'text': 'sukiya tomorrow cepat hungry', 'toke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-03-30 04:00:54+00:00</td>\n",
       "      <td>Hemarubeny30</td>\n",
       "      <td>So hungry ðŸ˜©</td>\n",
       "      <td>kajang,Malaysia</td>\n",
       "      <td>{'text': 'hungry', 'tokens': ['so', 'hungry'],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>2022-03-06 04:25:51+00:00</td>\n",
       "      <td>ffarhan26</td>\n",
       "      <td>@zulhilmi_imi Ah to ease your hunger\\nâœ¨at leas...</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>{'text': 'ah ease hunger least', 'tokens': ['a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>961</th>\n",
       "      <td>2022-02-26 15:19:57+00:00</td>\n",
       "      <td>sarahshdn</td>\n",
       "      <td>@cygaraga I may add some since I've been there...</td>\n",
       "      <td>Kuala Lumpur, Malaysia</td>\n",
       "      <td>{'text': 'may add since got kicked track covid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>962</th>\n",
       "      <td>2022-02-10 14:08:20+00:00</td>\n",
       "      <td>al_seng</td>\n",
       "      <td>@AzalinaOthmanS @MYParlimen To me, for a count...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'text': 'country country embrace democracy la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>963</th>\n",
       "      <td>2022-02-09 08:00:03+00:00</td>\n",
       "      <td>afmnurdin</td>\n",
       "      <td>dia mcm hunger game or maze runner show dah. S...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'text': 'dia mcm hunger game maze runner show...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>2022-02-06 23:41:36+00:00</td>\n",
       "      <td>7partsofsoul</td>\n",
       "      <td>Nak baca balik la The Hunger Games.</td>\n",
       "      <td>Damansara</td>\n",
       "      <td>{'text': 'nak baca balik la hunger game', 'tok...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>965 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Datetime      Username   \n",
       "0    2023-03-30 23:41:11+00:00    NumanAfifi  \\\n",
       "1    2023-03-30 12:07:36+00:00  marcosrahman   \n",
       "2    2023-03-30 10:12:34+00:00   LynnTweetss   \n",
       "3    2023-03-30 06:00:53+00:00       lzmnilz   \n",
       "4    2023-03-30 04:00:54+00:00  Hemarubeny30   \n",
       "..                         ...           ...   \n",
       "960  2022-03-06 04:25:51+00:00     ffarhan26   \n",
       "961  2022-02-26 15:19:57+00:00     sarahshdn   \n",
       "962  2022-02-10 14:08:20+00:00       al_seng   \n",
       "963  2022-02-09 08:00:03+00:00     afmnurdin   \n",
       "964  2022-02-06 23:41:36+00:00  7partsofsoul   \n",
       "\n",
       "                                                  Text   \n",
       "0    @ashvinmenon Turns out theyâ€™re the best when t...  \\\n",
       "1    this week is super hectic. literally the harde...   \n",
       "2                    Hungry or annoyed or just grumpy?   \n",
       "3                 sukiya!! tomorrow!! cepat!! hungry!!   \n",
       "4                                          So hungry ðŸ˜©   \n",
       "..                                                 ...   \n",
       "960  @zulhilmi_imi Ah to ease your hunger\\nâœ¨at leas...   \n",
       "961  @cygaraga I may add some since I've been there...   \n",
       "962  @AzalinaOthmanS @MYParlimen To me, for a count...   \n",
       "963  dia mcm hunger game or maze runner show dah. S...   \n",
       "964                Nak baca balik la The Hunger Games.   \n",
       "\n",
       "                   Location                                     Cleaned_Tweets  \n",
       "0              Kuala Lumpur  {'text': 'turn best hungry', 'tokens': ['turns...  \n",
       "1    Kuala Lumpur, Malaysia  {'text': 'week super hectic literally hardest ...  \n",
       "2         Kuala Lumpur City  {'text': 'hungry annoyed grumpy', 'tokens': ['...  \n",
       "3         Kuala Lumpur City  {'text': 'sukiya tomorrow cepat hungry', 'toke...  \n",
       "4           kajang,Malaysia  {'text': 'hungry', 'tokens': ['so', 'hungry'],...  \n",
       "..                      ...                                                ...  \n",
       "960                Malaysia  {'text': 'ah ease hunger least', 'tokens': ['a...  \n",
       "961  Kuala Lumpur, Malaysia  {'text': 'may add since got kicked track covid...  \n",
       "962                     NaN  {'text': 'country country embrace democracy la...  \n",
       "963                     NaN  {'text': 'dia mcm hunger game maze runner show...  \n",
       "964              Damansara   {'text': 'nak baca balik la hunger game', 'tok...  \n",
       "\n",
       "[965 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create a new column called \"Cleaned_Tweets\" by applying preprocessed tweets function to the 'Tweet' column\n",
    "new_df['Cleaned_Tweets'] = new_df['Text'].apply(lambda x: cleanedTweets(x))\n",
    "display(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd750cd",
   "metadata": {},
   "source": [
    "<h2>Data Cleaning</h2>\n",
    "\n",
    "Contents:\n",
    "1. Remove Non-English Tweets\n",
    "2. Remove Duplicate Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9221ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-English tweets\n",
    "processed_df = new_df[new_df['Cleaned_Tweets'].apply(lambda x: detect(x) == 'en')]\n",
    "display(processed_df)\n",
    "\n",
    "print('Number of rows left:', len(processed_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286f7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check if text is in English\n",
    "def is_english(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang == 'en'\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "# Remove non-English tweets\n",
    "processed_df = new_df[new_df['Cleaned_Tweets'].apply(is_english)]\n",
    "display(processed_df)\n",
    "print('Number of rows left:', len(processed_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5d8891",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Remove duplicate tweets in the dataset\n",
    "cleaned_df = processed_df.drop_duplicates(subset='Cleaned_Tweets')\n",
    "display(cleaned_df)\n",
    "\n",
    "print('Number of rows left:', len(cleaned_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cd098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the 'Text' and 'Cleaned_Tweets' columns\n",
    "#List of columns to drop\n",
    "columns_to_drop = [\"Tokens\", \"Filtered_Words\", \"Lemmatized_Words\", \"Stemmed_Words\"]\n",
    "\n",
    "#Drop the irrelavant columns mentioned in the list\n",
    "final_df = cleaned_df.drop(columns_to_drop, axis=1)\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb8b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the cleaned dataset\n",
    "cleaned_df.to_csv('CLEANED_Hungry_Malaysia (Coordinates) - Snscrape.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cea9cc2",
   "metadata": {},
   "source": [
    "## Data Cleaning ONLY<a name=\"Section1\"></a>\n",
    "\n",
    "This process does not include data preprocessing phase (Lemmatizing, Stemming, Tokenization, Removal of stop words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64259ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import contractions\n",
    "\n",
    "#to detect text language\n",
    "from langdetect import detect, LangDetectException\n",
    "\n",
    "#stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#Lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d55c52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load csv file containing tweets dataset\n",
    "def load_data():\n",
    "    data = pd.read_csv(r\"C:\\Users\\LENOVO\\Documents\\Degree Life\\FYP Journey\\Dataset\\Python Scraping\\Snscrape\\Feb 2022 - Mar 2023\\Hungry\\States\\[COMBINED] Hungry_States (Coordinates) - Snscrape.csv\")\n",
    "    return data\n",
    "\n",
    "tweet_df = load_data()\n",
    "display(tweet_df)\n",
    "\n",
    "print('Number of original rows:', len(tweet_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cc28c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unnecessary columns (ID)\n",
    "new_df = tweet_df.drop(['ID'], axis=1)\n",
    "display(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da71abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Cleaning process STARTS HERE\n",
    "\n",
    "def expand_contractions(text):\n",
    "    # Expand contractions\n",
    "    expanded_text = contractions.fix(text)\n",
    "    return expanded_text\n",
    "\n",
    "#clean the tweets with a function\n",
    "def cleanedTweets(text):\n",
    "    # Expand contractions\n",
    "    text = expand_contractions(text)\n",
    "    #lowercase text\n",
    "    text = text.lower()\n",
    "    # remove @ and links\n",
    "    text = ' '.join(re.sub(\"(@[A-Za-z0-9_]+|[^0-9A-Za-z \\t]) | (\\w+:\\/\\/\\S+)\",\" \",text).split())\n",
    "    #Remove repeating words\n",
    "    text = re.sub(r'\\@\\w+|\\#\\w+|\\d+','', text)\n",
    "    #Remove punctuation & numbers\n",
    "    punct = str.maketrans('','',string.punctuation)\n",
    "    text = text.translate(punct)\n",
    "    #Removing all URLS (hhtps and www)\n",
    "    text = re.sub(r'http\\S+','', text)\n",
    "    text = re.sub(r'www\\S+','', text)\n",
    "    # remove special characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd67a576",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create a new column called \"Cleaned_Tweets\" by applying preprocessed tweets function to the 'Tweet' column\n",
    "new_df['Cleaned_Tweets'] = new_df['Text'].apply(lambda x: cleanedTweets(x))\n",
    "display(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669c46f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-English tweets\n",
    "processed_df = new_df[new_df['Cleaned_Tweets'].apply(lambda x: detect(x) == 'en')]\n",
    "display(processed_df)\n",
    "\n",
    "print('Number of rows left:', len(processed_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45beaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check if text is in English\n",
    "def is_english(text):\n",
    "    try:\n",
    "        lang = detect(text)\n",
    "        return lang == 'en'\n",
    "    except LangDetectException:\n",
    "        return False\n",
    "\n",
    "# Remove non-English tweets\n",
    "processed_df = new_df[new_df['Cleaned_Tweets'].apply(is_english)]\n",
    "display(processed_df)\n",
    "print('Number of rows left:', len(processed_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db27cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Remove duplicate tweets in the dataset\n",
    "cleaned_df = processed_df.drop_duplicates(subset='Cleaned_Tweets')\n",
    "display(cleaned_df)\n",
    "\n",
    "print('Number of rows left:', len(cleaned_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b8f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the cleaned dataset\n",
    "cleaned_df.to_csv('CLEANED_Hungry_Malaysia (Coordinates) - Snscrape.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
