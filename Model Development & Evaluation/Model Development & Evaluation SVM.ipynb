{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "135845bb",
   "metadata": {},
   "source": [
    "<h1>Model Development Support Vector Machine (SVM)</h1>\n",
    "\n",
    "## Table of Contents<a name=\"TOC\"></a>\n",
    "\n",
    "1. [Splitting the Dataset Into Training and Testing Sets](#Section1)\n",
    "<br>First, separate the columns into dependent and independent variables (or features and labels). Then you split those variables into train and test sets.</br>\n",
    "\n",
    "2. [Feature Extraction](#Section2)\n",
    "<br>Includes document-term matirx (TF-IDF & BOW)</br>\n",
    "\n",
    "3. [Model Generation](#Section3)\n",
    "<br>Building SA Modelling using **SVM**</br>\n",
    "\n",
    "4. [Model Evaluation](#Section4)\n",
    "<br>Evaluate the SVM modelling based on performance metrics</br>\n",
    "\n",
    "5. [Visualization](#Section5)\n",
    "<br>Heatmaps and Stacked Bar Charts</br>\n",
    "\n",
    "Approach:\n",
    "1. Sentiment Analysis using SVM\n",
    "\n",
    "2. Predicting Sentiment using SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab4b282",
   "metadata": {},
   "source": [
    "**Importing libraries & dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a8310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import nltk\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50603d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv file containing tweets dataset (w/ sentiments)\n",
    "\n",
    "tweets_df = pd.read_csv(r\"C:\\Users\\LENOVO\\Documents\\Degree Life\\FYP Journey\\Dataset\\Sentiment Analysis\\V3 Harmonized [VADER & TextBlob]_All Keywords (Whole Malaysia).csv\")\n",
    "display(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4209f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop irrelvant columns for modelling purposes\n",
    "#Irrelevant columns = \"Datetime\", \"Username\", \"Location\"\n",
    "new_df = tweets_df.drop(['Datetime', 'Username','Location'], axis=1)\n",
    "\n",
    "# Create a list of the column names in the desired order (VADER & TextBlob)\n",
    "#cols = ['Cleaned_Tweets', 'Sentiment Score','Sentiment']\n",
    "\n",
    "# Create a list of the column names in the desired order (Harmonized)\n",
    "cols = ['Cleaned_Tweets', 'Harmonized_Score','Harmonized_Label', 'Risk_Label']\n",
    "\n",
    "# Rearrange the columns in the dataset\n",
    "new_df = new_df[cols]\n",
    "\n",
    "display(new_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b32459b",
   "metadata": {},
   "source": [
    "## 1. Splitting the Dataset Into Training and Testing Sets<a name=\"Section1\"></a>\n",
    "\n",
    "Use scikit-learn to randomly split the dataset into training and testing sets. You will use the training set to train the model to classify the sentiments of the reviews. And you will use the test set to access how good the model is at classifying new unseen reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434dacde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the features and labels\n",
    "features = new_df['Cleaned_Tweets'].values\n",
    "labels = new_df['Harmonized_Label'].values\n",
    "\n",
    "# Use LabelEncoder to convert labels to numerical values\n",
    "# Positive [1] OR Negative [0]\n",
    "encoder = LabelEncoder()\n",
    "encoded_labels = encoder.fit_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b273c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "# Remember to modify test size each time you're trying to run a new model!!\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, encoded_labels, \n",
    "                                                                            test_size=0.3, random_state=42)\n",
    "\n",
    "train_features.shape, test_features.shape, train_labels.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ec95bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert train_features and train_labels back to Pandas DataFrames\n",
    "train_data = pd.DataFrame({'Cleaned_Tweets': train_features, 'Sentiment': train_labels})\n",
    "#train_data = pd.concat([train_features.reset_index(drop=True), train_labels.reset_index(drop=True)], axis=1)\n",
    "display(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d924568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert test_features and test_labels back to Pandas DataFrames\n",
    "test_data = pd.DataFrame({'Cleaned_Tweets': test_features, 'Sentiment': test_labels})\n",
    "display(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf359af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the train data into CSV file\n",
    "train_data.to_csv('(70-30)TextBlob train_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79975579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the test data into CSV file\n",
    "test_data.to_csv('(70-30)TextBlob test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661bce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#view tweet length in train data and test data\n",
    "\n",
    "length_train = train_data['Cleaned_Tweets'].str.len()\n",
    "length_test = test_data['Cleaned_Tweets'].str.len()\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.hist(length_train, bins=50, label=\"Train_tweets\", color = \"darkblue\")\n",
    "plt.hist(length_test, bins=50, label='Test_tweets', color = \"skyblue\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83415783",
   "metadata": {},
   "source": [
    "**To check if the dataset is balanced**\n",
    "\n",
    "Source: https://towardsdatascience.com/fine-grained-sentiment-analysis-in-python-part-1-2697bb111ed4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c3a8c8",
   "metadata": {},
   "source": [
    "**Train data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c5e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ax = train_data['Sentiment'].value_counts(sort=False).plot(kind='barh')\n",
    "ax.set_xlabel(\"Number of Samples in Training Set\")\n",
    "ax.set_ylabel(\"Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a94aa3",
   "metadata": {},
   "source": [
    "**Test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed84867a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ax = test_data['Sentiment'].value_counts(sort=False).plot(kind='barh')\n",
    "ax.set_xlabel(\"Number of Samples in Testing Set\")\n",
    "ax.set_ylabel(\"Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad2c0e5",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction<a name=\"Section2\"></a>\n",
    "\n",
    "Using TF-IDF or BOW to vectorize text reviews --> numbers\n",
    "\n",
    "A.[TF-IDF](#Section6)\n",
    "\n",
    "B.[BOW](#Section7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543e7291",
   "metadata": {},
   "source": [
    "### A. TF-IDF<a name=\"Section6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e2364f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with an empty string\n",
    "train_features = np.where(pd.isnull(train_features), '', train_features)\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()  # For TF-IDF\n",
    "\n",
    "# Fit the vectorizer on the training data and transform the training features\n",
    "train_features_vectorized = vectorizer.fit_transform(train_features)\n",
    "\n",
    "# Transform the testing features using the trained vectorizer\n",
    "test_features_vectorized = vectorizer.transform(test_features)\n",
    "\n",
    "train_features_vectorized.shape, test_features_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05614617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sparse matrix to a dense matrix and create a DataFrame\n",
    "train_features_df = pd.DataFrame(train_features_vectorized.toarray(), columns=vectorizer.get_feature_names())\n",
    "test_features_df = pd.DataFrame(test_features_vectorized.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Display the feature vectors\n",
    "print(\"Training Features:\\n\", train_features_df)\n",
    "print(\"Testing Features:\\n\", test_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1da07b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train_features vectors to CSV files\n",
    "train_features_df.to_csv('TF-IDF (70-30)- TextBlob train_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48d766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the test_features vectors to CSV files\n",
    "test_features_df.to_csv('TF-IDF (70-30)- TextBlob test_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8427f730",
   "metadata": {},
   "source": [
    "### B. BOW<a name=\"Section7\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182d92c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with an empty string\n",
    "train_features = np.where(pd.isnull(train_features), '', train_features)\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()  # For BoW\n",
    "\n",
    "# Fit the vectorizer on the training data and transform the training features\n",
    "train_features_vectorized = vectorizer.fit_transform(train_features)\n",
    "\n",
    "# Transform the testing features using the trained vectorizer\n",
    "test_features_vectorized = vectorizer.transform(test_features)\n",
    "\n",
    "train_features_vectorized.shape, test_features_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5666d056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sparse matrix to a dense matrix and create a DataFrame\n",
    "train_features_df = pd.DataFrame(train_features_vectorized.toarray(), columns=vectorizer.get_feature_names())\n",
    "test_features_df = pd.DataFrame(test_features_vectorized.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# Display the feature vectors\n",
    "print(\"Training Features:\\n\", train_features_df)\n",
    "print(\"Testing Features:\\n\", test_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d582638a",
   "metadata": {},
   "source": [
    "## 3. Model Generation<a name=\"Section3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5005fac",
   "metadata": {},
   "source": [
    "## 3.1 Train model using SVM<a name=\"Section6\"></a>\n",
    "\n",
    "\n",
    "\n",
    "How to train a SVM classifier?\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab80599",
   "metadata": {},
   "source": [
    "**To save time, jump here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb6e46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case you wanna use the loaded classifier model from file\n",
    "# Use this code to perform prediction\n",
    "predictions = loaded_model.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6770cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training dataset from CSV file\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "\n",
    "# Load the testing dataset from CSV file\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Load the TF-IDF vectorized features for training from CSV file\n",
    "train_features = pd.read_csv('train_features.csv')\n",
    "\n",
    "# Load the TF-IDF vectorized features for testing from CSV file\n",
    "test_features = pd.read_csv('test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11407709",
   "metadata": {},
   "source": [
    "**Training SVM Classifier (Linear)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f7a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "start_time = time.time()\n",
    "svm_classifier.fit(train_features_vectorized, train_labels)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the runtime of testing the classifier\n",
    "print(f\"Testing time: {end_time - start_time} seconds\")\n",
    "display(svm_classifier.get_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae79a6b",
   "metadata": {},
   "source": [
    "**Predict labels of test data**\n",
    "<br>\n",
    "Use the print() function to display the test_predictions array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8182ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of the sentiment labels\n",
    "sentiment_labels = ['Positive', 'Negative']\n",
    "\n",
    "\n",
    "# Predict the labels of the test data\n",
    "start_time = time.time()\n",
    "test_predictions = svm_classifier.predict(test_features_vectorized)\n",
    "end_time = time.time()\n",
    "\n",
    "# Print the runtime of predicting the labels\n",
    "print(f\"Prediction time: {end_time - start_time} seconds\")\n",
    "\n",
    "# Convert the numeric labels back to sentiment labels\n",
    "actual_sentiments = encoder.inverse_transform(test_labels)\n",
    "predicted_sentiments = encoder.inverse_transform(test_predictions)\n",
    "\n",
    "# Print the predicted labels\n",
    "display(test_predictions, predicted_sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d532ced2",
   "metadata": {},
   "source": [
    "**Print results of predicted labels using DataFrame**\n",
    "\n",
    "Create a new DataFrame that combines the test data with the predicted labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f33b132",
   "metadata": {},
   "source": [
    "**For TextBlob & VADER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce641cde",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame with the test data and predicted labels\n",
    "results_df = pd.DataFrame({'Text': test_data['Cleaned_Tweets'], 'actual_sentiment': actual_sentiments, \n",
    "                           'predicted_sentiment': predicted_sentiments})\n",
    "\n",
    "# Print the DataFrame\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00df53c",
   "metadata": {},
   "source": [
    "**For Harmonized**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46598c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map sentiment labels to risk labels\n",
    "sentiment_labels = ['Positive', 'Weakly Negative', 'Mild Negative', 'Strongly Negative']\n",
    "risk_labels = ['Low Risk', 'Mild Risk', 'Moderate Risk', 'Severe Risk']\n",
    "label_mapping = {sentiment_labels[i]: risk_labels[i] for i in range(len(sentiment_labels))}\n",
    "\n",
    "# Apply reassignment to the actual and predicted sentiment labels\n",
    "results_df['actual_risk'] = results_df['actual_sentiment'].map(label_mapping)\n",
    "results_df['predicted_risk'] = results_df['predicted_sentiment'].map(label_mapping)\n",
    "\n",
    "# Print the dataframe\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c620dce2",
   "metadata": {},
   "source": [
    "**Print results of predicted labels using DataFrame**\n",
    "\n",
    "Create a new DataFrame that combines the test data with the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fe18b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save results into CSV file\n",
    "results_df.to_csv('SVM_BOW (80-20)- Predict Label Modelling Results [VADER].csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27fdbf7",
   "metadata": {},
   "source": [
    "<h3>Pickling the Model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fd7974",
   "metadata": {},
   "source": [
    "If you still want to see the full output of the classifier object, you can try using the pickle module to save the classifier object to a file and then load it back into memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22a763b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the classifier object to a file\n",
    "with open('SVM_classifier (TF-IDF_VADER - 70-30).pkl', 'wb') as file:\n",
    "    pickle.dump(svm_classifier, file)\n",
    "\n",
    "# Print the classifier object\n",
    "print(svm_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6430d245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the classifier object from the file\n",
    "with open('SVM_classifier.pkl', 'rb') as file:\n",
    "    nb_classifier = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b95e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case you wanna use the loaded classifier model from file\n",
    "# Use this code to perform prediction\n",
    "predictions = loaded_model.predict(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da1879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training dataset from CSV file\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "\n",
    "# Load the testing dataset from CSV file\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "# Load the TF-IDF vectorized features for training from CSV file\n",
    "train_features = pd.read_csv('train_features.csv')\n",
    "\n",
    "# Load the TF-IDF vectorized features for testing from CSV file\n",
    "test_features = pd.read_csv('test_features.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91112225",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation<a name=\"Section4\"></a>\n",
    "\n",
    "**A. Evaluation Metrics:**\n",
    "\n",
    "1. Accuracy\n",
    "<br>\n",
    "2. Precision\n",
    "<br>\n",
    "3. F1 Score\n",
    "<br> Due to an imbalance classes, F1 score was metric was used </br>\n",
    "4. Recall\n",
    "\n",
    "**B. K-Fold Cross Validation**\n",
    "\n",
    "Using k-fold (k = 10)\n",
    "<br></br>\n",
    "Part of code retrieved from here:\n",
    "https://github.com/ThinamXx/Twitter..Sentiment..Analysis/blob/master/Twitter%20Sentiment%20Analysis.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b001a1f",
   "metadata": {},
   "source": [
    "<h2> A. Evaluation Metrics </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff30ff4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Using the fitted model to make predictions on testing data\n",
    "# Predict the labels of the test data\n",
    "test_predictions = svm_classifier.predict(test_features_vectorized)\n",
    "\n",
    "# Print the predicted labels\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a79e44",
   "metadata": {},
   "source": [
    "**Checking the accuracy in Testing Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f86557",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# A) VADER & textBlob DATASET\n",
    "\n",
    "# Create a classification report\n",
    "classification = classification_report(results_df['actual_sentiment'], results_df['predicted_sentiment'])\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(results_df['actual_sentiment'], results_df['predicted_sentiment'])\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "# Calculate the precision of the classifier\n",
    "precision = precision_score(test_labels, test_predictions, average='weighted')\n",
    "\n",
    "# Calculate the recall of the classifier\n",
    "recall = recall_score(test_labels, test_predictions, average='weighted')\n",
    "\n",
    "# Calculate the F1 score of the classifier\n",
    "f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "\n",
    "# Calculate the confusion matrix of the classifier\n",
    "confusion_mat = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "# Define the labels for the confusion matrix\n",
    "sentiment_labels = ['Food Secured', 'Food Insecure']\n",
    "\n",
    "# Create a DataFrame with the confusion matrix and labels\n",
    "confusion_df = pd.DataFrame(confusion_mat, index=sentiment_labels, columns=sentiment_labels)\n",
    "\n",
    "# Print the evaluation metrics & confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(\"Accuracy:\", accuracy * 100, \"%\")\n",
    "print(\"Precision:\", precision * 100, \"%\")\n",
    "print(\"Recall:\", recall * 100, \"%\")\n",
    "print(\"F1 Score:\", f1 * 100, \"%\")\n",
    "display(confusion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad063e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a heatmap of the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', xticklabels=sentiment_labels, yticklabels=sentiment_labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5027b939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# B) HARMONIZED DATASET\n",
    "\n",
    "# Create a classification report\n",
    "classification = classification_report(results_df['actual_risk'], results_df['predicted_risk'])\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(results_df['actual_risk'], results_df['predicted_risk'])\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "# Calculate the precision of the classifier\n",
    "precision = precision_score(test_labels, test_predictions, average='weighted')\n",
    "\n",
    "# Calculate the recall of the classifier\n",
    "recall = recall_score(test_labels, test_predictions, average='weighted')\n",
    "\n",
    "# Calculate the F1 score of the classifier\n",
    "f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "\n",
    "# Calculate the confusion matrix of the classifier\n",
    "confusion_mat = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "# Define the labels for the confusion matrix\n",
    "risk_labels = ['Low Risk', 'Mild Risk', 'Moderate Risk', 'Severe Risk']\n",
    "\n",
    "# Create a DataFrame with the confusion matrix and labels\n",
    "confusion_df = pd.DataFrame(confusion_mat, index=risk_labels, columns=risk_labels)\n",
    "\n",
    "# Print the evaluation metrics & confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(\"Accuracy:\", accuracy * 100, \"%\")\n",
    "print(\"Precision:\", precision * 100, \"%\")\n",
    "print(\"Recall:\", recall * 100, \"%\")\n",
    "print(\"F1 Score:\", f1 * 100, \"%\")\n",
    "display(confusion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5591925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', xticklabels=risk_labels, yticklabels=risk_labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48adad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the evaluation metrics\n",
    "evaluation_results = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Score': [accuracy, precision, recall, f1]\n",
    "}\n",
    "\n",
    "df_evaluation = pd.DataFrame(evaluation_results)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix\n",
    "confusion_df = pd.DataFrame(confusion_mat, columns=sentiment_labels, index=sentiment_labels)\n",
    "\n",
    "# Save the evaluation DataFrame to a CSV file\n",
    "df_evaluation.to_csv('[VADER] SVM TF-IDF (70-30) Inital Model_Evaluation.csv', index=False)\n",
    "\n",
    "# Save the confusion matrix DataFrame to a CSV file\n",
    "confusion_df.to_csv('[VADER] SVM TF-IDF (70-30) Inital Model_Confusion_Matrix.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3c4306",
   "metadata": {},
   "source": [
    "<h3>ROC Curve</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0981f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Calculate the predicted probabilities for the positive class (1)\n",
    "predicted_probabilities = svm_classifier.decision_function(test_features_vectorized)\n",
    "\n",
    "# Print the predicted labels\n",
    "print(predicted_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6089b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create ROC curve\n",
    "fpr, tpr, thresholds = roc_curve(test_labels, predicted_probabilities)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot the ROC curve\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85668857",
   "metadata": {},
   "source": [
    "## 2. K-Fold Cross Validation<a name=\"Section2\"></a>\n",
    "Back to [Top Page](#TOC)\n",
    "\n",
    "Use the cross_val_score() function from sklearn.model_selection to evaluate the performance of the classifier using 5-fold cross-validation. \n",
    "<br></br>\n",
    "The cross_val_score() function takes the classifier, the feature vectors, the labels, and the number of folds as input, and returns an array of scores for each fold\n",
    "\n",
    "**Output: Cross-validation scores for each fold + Average cross-validation score**\n",
    "\n",
    "Note: cv = k (k-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a99883",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform k-fold cross-validation and obtain the scores for each fold\n",
    "scores = cross_val_score(svm_classifier, train_features_vectorized, train_labels, cv=10)\n",
    "\n",
    "# Print the accuracy for each fold\n",
    "for fold, score in enumerate(scores):\n",
    "    print(f\"Fold {fold+1}: {score}\")\n",
    "\n",
    "# Calculate and print the mean accuracy and standard deviation\n",
    "mean_accuracy = scores.mean()\n",
    "std_deviation = scores.std()\n",
    "rmse = np.sqrt(-scores)\n",
    "print(f\"Mean accuracy: {mean_accuracy}\")\n",
    "print(f\"Standard deviation: {std_deviation}\")\n",
    "print(\"RMSE values: \", np.round(rmse, 2))\n",
    "print(\"RMSE average: \", np.round(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af693643",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Evaluate the performance of SVM classifier using cross-validation\n",
    "scores = cross_val_score(svm_classifier, train_features_vectorized, train_labels, cv=10)\n",
    "\n",
    "#rmse = np.sqrt(-scores)\n",
    "\n",
    "# Print the cross-validation scores\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Average cross-validation score: {scores.mean()}\")\n",
    "#print(\"RMSE values: \", np.round(rmse, 2))\n",
    "#print(\"RMSE average: \", np.round(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6248dcc1",
   "metadata": {},
   "source": [
    "## 3. Hyperparameter Tuning<a name=\"Section4\"></a>\n",
    "\n",
    "Back to [Top Page](#TOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb76fd",
   "metadata": {},
   "source": [
    "**How to find the BEST hyperparameters for SVM classifier**\n",
    "\n",
    "The GridSearchCV() function takes the classifier, the hyperparameters, and the number of folds as input, and returns the best hyperparameters and the corresponding score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4339c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV #Perform grid search over hyperparameters\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create instance of SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "\n",
    "# Define the hyperparameters to search over\n",
    "hyperparameters = {'C': [0.1, 0.5, 1.0, 2.0, 5.0, 10.0]}\n",
    "\n",
    "# Use grid search to find the best hyperparameters for the classifier\n",
    "grid_search = GridSearchCV(svm_classifier, hyperparameters, cv=10)\n",
    "grid_search.fit(train_features_vectorized, train_labels)\n",
    "\n",
    "# Print the best hyperparameters and the corresponding score\n",
    "print(f\"Best hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ac9b3",
   "metadata": {},
   "source": [
    "We then print the best hyperparameters and the corresponding score using the print() function. The resulting output will show the best hyperparameters found by the grid search and the corresponding score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6681313d",
   "metadata": {},
   "source": [
    "After finding the best hyperparameters, you can process to train and evaluate SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e83a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with an empty string\n",
    "train_features = np.where(pd.isnull(train_features), '', train_features)\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = TfidfVectorizer()  # For TF-IDF\n",
    "#vectorizer = CountVectorizer()  # For BoW\n",
    "\n",
    "# Fit the vectorizer on the training data and transform thzzzzze training features\n",
    "train_features_vectorized = vectorizer.fit_transform(train_features)\n",
    "\n",
    "# Transform the testing features using the trained vectorizer\n",
    "test_features_vectorized = vectorizer.transform(test_features)\n",
    "\n",
    "train_features_vectorized.shape, test_features_vectorized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f31a146",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# Remember to modify test size each time you're trying to run a new model!!\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, encoded_labels, \n",
    "                                                                            test_size=0.2, random_state=42)\n",
    "# Replace NaN values with an empty string\n",
    "train_features = np.where(pd.isnull(train_features), '', train_features)\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = TfidfVectorizer()  # For TF-IDF\n",
    "#vectorizer = CountVectorizer()  # For BoW\n",
    "\n",
    "# Fit the vectorizer on the training data and transform the training features\n",
    "train_features_vectorized = vectorizer.fit_transform(train_features)\n",
    "\n",
    "# Transform the testing features using the trained vectorizer\n",
    "test_features_vectorized = vectorizer.transform(test_features)\n",
    "\n",
    "# Create instance of SVM classifier with best hyperparameters\n",
    "svm_classifier = SVC(kernel='linear', C=2.0)\n",
    "\n",
    "# Train a SVM classifier on the training data\n",
    "svm_classifier.fit(train_features_vectorized, train_labels)\n",
    "\n",
    "# Predict the labels of the test data\n",
    "test_predictions = svm_classifier.predict(test_features_vectorized)\n",
    "\n",
    "# Evaluate the performance of the classifier on the test data\n",
    "confusion = confusion_matrix(test_labels, test_predictions)\n",
    "report = classification_report(test_labels, test_predictions)\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "# Define the labels for the confusion matrix\n",
    "sentiment_labels = ['Food Secured', 'Food Insecure']\n",
    "\n",
    "# Print the confusion matrix, classification report, and accuracy score\n",
    "print(f\"Confusion matrix:\\n{confusion}\")\n",
    "print(f\"Classification report:\\n{report}\")\n",
    "print(f\"Accuracy score: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db3dbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A) VADER & textBlob DATASET\n",
    "\n",
    "# Create a classification report\n",
    "classification = classification_report(results_df['actual_sentiment'], results_df['predicted_sentiment'])\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(results_df['actual_sentiment'], results_df['predicted_sentiment'])\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "# Calculate the precision of the classifier\n",
    "precision = precision_score(test_labels, test_predictions, average='weighted')\n",
    "\n",
    "# Calculate the recall of the classifier\n",
    "recall = recall_score(test_labels, test_predictions, average='weighted')\n",
    "\n",
    "# Calculate the F1 score of the classifier\n",
    "f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "\n",
    "# Calculate the confusion matrix of the classifier\n",
    "confusion_mat = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "# Define the labels for the confusion matrix\n",
    "sentiment_labels = ['Food Secured', 'Food Insecure']\n",
    "\n",
    "# Create a DataFrame with the confusion matrix and labels\n",
    "confusion_df = pd.DataFrame(confusion_mat, index=sentiment_labels, columns=sentiment_labels)\n",
    "\n",
    "# Print the evaluation metrics & confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(\"Accuracy:\", accuracy * 100, \"%\")\n",
    "print(\"Precision:\", precision * 100, \"%\")\n",
    "print(\"Recall:\", recall * 100, \"%\")\n",
    "print(\"F1 Score:\", f1 * 100, \"%\")\n",
    "display(confusion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faef7bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', xticklabels=sentiment_labels, yticklabels=sentiment_labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c5696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For HARMONIZED\n",
    "\n",
    "# Create a classification report\n",
    "classification = classification_report(results_df['actual_risk'], results_df['predicted_risk'])\n",
    "\n",
    "# Calculate accuracy score\n",
    "accuracy = accuracy_score(results_df['actual_risk'], results_df['predicted_risk'])\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "# Calculate the precision of the classifier\n",
    "precision = precision_score(test_labels, test_predictions, average='weighted')\n",
    "\n",
    "# Calculate the recall of the classifier\n",
    "recall = recall_score(test_labels, test_predictions, average='weighted')\n",
    "\n",
    "# Calculate the F1 score of the classifier\n",
    "f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "\n",
    "# Calculate the confusion matrix of the classifier\n",
    "confusion_mat = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "# Define the labels for the confusion matrix\n",
    "risk_labels = ['Low Risk', 'Mild Risk', 'Moderate Risk', 'Severe Risk']\n",
    "\n",
    "# Create a DataFrame with the confusion matrix and labels\n",
    "confusion_df = pd.DataFrame(confusion_mat, index=risk_labels, columns=risk_labels)\n",
    "\n",
    "# Print the evaluation metrics & confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_mat)\n",
    "\n",
    "print(\"\\nEvaluation Metrics:\")\n",
    "print(\"Accuracy:\", accuracy * 100, \"%\")\n",
    "print(\"Precision:\", precision * 100, \"%\")\n",
    "print(\"Recall:\", recall * 100, \"%\")\n",
    "print(\"F1 Score:\", f1 * 100, \"%\")\n",
    "display(confusion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7848dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues', xticklabels=risk_labels, yticklabels=risk_labels)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a82677",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Calculate the accuracy of the classifier\n",
    "accuracy = accuracy_score(test_labels, test_predictions)\n",
    "\n",
    "# Calculate the precision of the classifier\n",
    "precision = precision_score(test_labels, test_predictions, average='weighted')\n",
    "\n",
    "# Calculate the recall of the classifier\n",
    "recall = recall_score(test_labels, test_predictions, average='weighted')\n",
    "\n",
    "# Calculate the F1 score of the classifier\n",
    "f1 = f1_score(test_labels, test_predictions, average='weighted')\n",
    "\n",
    "# Calculate the confusion matrix of the classifier\n",
    "confusion_mat = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "# Define the labels for the confusion matrix\n",
    "labels = ['True Negative (TN)', 'False Positive (FP)', 'False Negative (FN)', 'True Positive (TP)']\n",
    "\n",
    "# Create a new confusion matrix with the labels\n",
    "confusion_mat_labeled = np.empty((2,2), dtype=int)\n",
    "confusion_mat_labeled[0,0] = confusion_mat[0,0] # True Negative\n",
    "confusion_mat_labeled[0,1] = confusion_mat[0,1] # False Positive\n",
    "confusion_mat_labeled[1,0] = confusion_mat[1,0] # False Negative\n",
    "confusion_mat_labeled[1,1] = confusion_mat[1,1] # True Positive\n",
    "\n",
    "# Create a DataFrame with the confusion matrix and labels\n",
    "confusion_df = pd.DataFrame(confusion_mat_labeled, index=labels[:2], columns=labels[2:])\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Evaluation Metrics:\")\n",
    "print(\"Accuracy:\", accuracy * 100, \"%\")\n",
    "print(\"Precision:\", precision * 100, \"%\")\n",
    "print(\"Recall:\", recall * 100, \"%\")\n",
    "print(\"F1 Score:\", f1 * 100, \"%\")\n",
    "display(\"Confusion Matrix:\", confusion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83d07a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the evaluation metrics (After tuning)\n",
    "evaluation_results = {\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "    'Score': [accuracy, precision, recall, f1]\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the evaluation results\n",
    "df_evaluation = pd.DataFrame(evaluation_results)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix\n",
    "confusion_df = pd.DataFrame(confusion_mat, columns=['False Negative', 'False Positive'], index=['True Negative', 'True Positive'])\n",
    "\n",
    "# Concatenate the evaluation DataFrame and confusion DataFrame\n",
    "results_df = pd.concat([df_evaluation, confusion_df], axis=0)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('[VADER] SVM BOW (80-20) Post Hypertuning Model_Evaluation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf6df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Harmonized Sentiments Dataset\n",
    "# Create a DataFrame from the confusion matrix\n",
    "confusion_df = pd.DataFrame(confusion_mat, columns=sentiment_labels, index=sentiment_labels)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('[Harmonized] SVM TF-IDF (80-20) Initial Model_Evaluation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c009730c",
   "metadata": {},
   "source": [
    "## 5. Predict Sentiment of New text Data<a name=\"Section5\"></a>\n",
    "Back to [Top Page](#TOC)\n",
    "\n",
    "Using the trained model classifier, we can predict the sentiment of new text data\n",
    "<br>\n",
    "\n",
    "Positive [1] - Food Secured\n",
    "\n",
    "Negative [0] - Food Insecure\n",
    "\n",
    "A) [VADER/TextBlob Dataset](#Section14)\n",
    "<br>\n",
    "B) [Harmonized Dataset](#Section15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332f588f",
   "metadata": {},
   "source": [
    "### A) VADER/TextBlob Dataset<a name=\"Section14\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da25b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "#vectorizer = TfidfVectorizer()  # For TF-IDF\n",
    "vectorizer = CountVectorizer() # For BOW\n",
    "\n",
    "# Vectorize the training data\n",
    "train_features_vectorized = vectorizer.fit_transform(train_features)\n",
    "\n",
    "# Create instance of SVM classifier with best hyperparameters\n",
    "svm_classifier = SVC(kernel='linear', C=1.0, probability=True)  # Set probability=True\n",
    "\n",
    "# Train a Naive Bayes classifier on the training data\n",
    "svm_classifier.fit(train_features_vectorized, train_labels)\n",
    "\n",
    "# Predict the sentiment of new text data\n",
    "new_data = [\n",
    "    \"I'm so angry about the high food prices! It's making it so hard for me to feed my family.\",\n",
    "    \"I'm so grateful for the food banks and other organizations that are helping to feed people who are struggling. They're making a real difference.\",\n",
    "    \"I'm so worried about the future of food security. Climate change is making it harder to grow food, and more people are going hungry.\",\n",
    "    \"I'm so inspired by the work of food banks and other organizations that are fighting hunger. They're making a real difference in people's lives.\",\n",
    "    \"I'm hopeful that we can create a world where everyone has access to the food they need to live a healthy and productive life\",\n",
    "    \"I'm working part-time and I'm not sure if I'll be able to keep my job.\",\n",
    "    \"I'm not sure if I'll be able to afford to pay my rent this month.\"\n",
    "]\n",
    "\n",
    "new_data_vectorized = vectorizer.transform(new_data)\n",
    "new_data_predictions = svm_classifier.predict(new_data_vectorized)\n",
    "new_data_sentiment_scores = svm_classifier.predict_proba(new_data_vectorized)[:, 1]  # Positive sentiment score\n",
    "\n",
    "# Print the predicted sentiment + sentiment scores for the new data\n",
    "for i in range(len(new_data)):\n",
    "    print(f\"Text: {new_data[i]}\")\n",
    "    sentiment_label = \"Positive (Food Secured)\" if new_data_predictions[i] == 1 else \"Negative (Food Insecure)\"\n",
    "    print(f\"Predicted sentiment: {sentiment_label}\")\n",
    "    print(f\"Sentiment score: {new_data_sentiment_scores[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5715af83",
   "metadata": {},
   "source": [
    "**Save the new data results into CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f1c581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to store the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Text': new_data,\n",
    "    'Predicted Sentiment': new_data_predictions,\n",
    "    'Sentiment Score': new_data_sentiment_scores\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "results_df.to_csv('[SAMPLE] [SVM] BOW Harmonized (80-20) new_data_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14534ff",
   "metadata": {},
   "source": [
    "### B) Harmonized Dataset<a name=\"Section15\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1306bc94",
   "metadata": {},
   "source": [
    "**Predict sentiment & FI Risk Category of New Text Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d91b4f",
   "metadata": {},
   "source": [
    "Go [here for the immediate solution](#Section16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a527429c",
   "metadata": {},
   "source": [
    "### Immediate Solution for Predicting FI Risk <a name=\"Section16\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5696851",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the risk category mapping\n",
    "risk_category_mapping = {\n",
    "    0: \"\\033[1;32mLow Risk\\033[0m\",  # Green\n",
    "    1: \"\\033[1;33mMild Risk\\033[0m\", # Yellow\n",
    "    2: \"\\033[1;31mModerate Risk\\033[0m\", # Orange\n",
    "    3: \"\\033[1;31mSevere Risk\\033[0m\"  # Red\n",
    "}\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "#vectorizer = TfidfVectorizer() # For TF-IDF\n",
    "vectorizer = CountVectorizer() # For BOW\n",
    "\n",
    "# Vectorize the training data\n",
    "features_vectorized = vectorizer.fit_transform(train_features)\n",
    "\n",
    "# Create instance of hypertuned SVM classifier with best hyperparameters\n",
    "svm_classifier = SVC(kernel='linear', C=1.0, probability=True)  # Set probability=True\n",
    "\n",
    "# Train a Multinomial Logistic Regression classifier on the training data\n",
    "svm_classifier.fit(train_features_vectorized, train_labels)\n",
    "\n",
    "# Function to predict sentiment, sentiment score, and FI risk category of new text data\n",
    "def predict_sentiment_and_fi_risk(text):\n",
    "    # Vectorize the new text data\n",
    "    new_text_vectorized = vectorizer.transform([text])\n",
    "\n",
    "    # Predict the sentiment using the trained model\n",
    "    sentiment = svm_classifier.predict(new_text_vectorized)[0]\n",
    "\n",
    "    # Predict the sentiment score using the trained model\n",
    "    sentiment_score = np.max(svm_classifier.predict_proba(new_text_vectorized))\n",
    "\n",
    "    # Assign the FI risk category based on sentiment and sentiment score\n",
    "    if sentiment == 1:  # Positive sentiment (Food Secured)\n",
    "        fi_sentiment = \"Positive\"\n",
    "        fi_risk = risk_category_mapping[0]  # Low Risk\n",
    "    else:  # Negative sentiment\n",
    "        sentiment_score *= -1  # Multiply the sentiment score by -1 for Negative sentiment (Food Insecure)\n",
    "        \n",
    "        if (sentiment_score > -1.0) and (sentiment_score <= -0.6):\n",
    "            fi_sentiment = \"Negative (Strongly Negative)\"\n",
    "            fi_risk = risk_category_mapping[3]  # Severe Risk\n",
    "        elif (sentiment_score > -0.6) and (sentiment_score <= -0.3):\n",
    "            fi_sentiment = \"Negative (Mild Negative)\"\n",
    "            fi_risk = risk_category_mapping[2]  # Moderate Risk\n",
    "        else:\n",
    "            fi_sentiment = \"Negative (Weakly Negative)\"\n",
    "            fi_risk = risk_category_mapping[1]  # Mild Risk\n",
    "\n",
    "    return fi_sentiment, sentiment_score, fi_risk\n",
    "\n",
    "\n",
    "# Predict the sentiment, sentiment score, and FI risk for the new text data\n",
    "new_data = [\n",
    "    \"I'm so angry about the high food prices! It's making it so hard for me to feed my family.\",\n",
    "    \"I'm so grateful for the food banks and other organizations that are helping to feed people who are struggling. They're making a real difference.\",\n",
    "    \"I'm so worried about the future of food security. Climate change is making it harder to grow food, and more people are going hungry.\",\n",
    "    \"I'm so inspired by the work of food banks and other organizations that are fighting hunger. They're making a real difference in people's lives.\",\n",
    "    \"I'm hopeful that we can create a world where everyone has access to the food they need to live a healthy and productive life\",\n",
    "    \"I'm working part-time and I'm not sure if I'll be able to keep my job.\",\n",
    "    \"I'm not sure if I'll be able to afford to pay my rent this month.\"\n",
    "]\n",
    "\n",
    "final_results = []\n",
    "for text in new_data:\n",
    "    fi_sentiment, sentiment_score, fi_risk = predict_sentiment_and_fi_risk(text)\n",
    "    print(\"Text:\", text)\n",
    "    print(\"Predicted sentiment:\", fi_sentiment)\n",
    "    print(\"Sentiment score:\", sentiment_score)\n",
    "    print(\"FI Risk:\", fi_risk)\n",
    "    print()\n",
    "    \n",
    "    # Store the results in a dictionary\n",
    "    final_result = {\n",
    "        \"Text\": text,\n",
    "        \"Predicted Sentiment\": fi_sentiment,\n",
    "        \"Sentiment Score\": sentiment_score,\n",
    "        \"FI Risk\": fi_risk\n",
    "    }\n",
    "    final_results.append(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031ee55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the results\n",
    "final_results_df = pd.DataFrame(final_results)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(final_results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8483fba9",
   "metadata": {},
   "source": [
    "**Save the new data results into CSV file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa135a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "final_results_df.to_csv('[SAMPLE] [SVM] BOW Harmonized (80-20) final new_data_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e96c7b",
   "metadata": {},
   "source": [
    "**ALTERNATE CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f06a56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe from the results\n",
    "columns = [\"Text\", \"Sentiment\", \"Sentiment Score\", \"FI Risk\"]\n",
    "final_results_df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# Assign colors to FI Risk categories\n",
    "color_mapping = {\n",
    "    \"Low\": \"\\033[92m\",              # Green\n",
    "    \"Mild\": \"\\033[93m\",             # Yellow\n",
    "    \"Moderate\": \"\\033[38;5;208m\",   # Light Orange\n",
    "    \"High\": \"\\033[91m\"              # Red\n",
    "}\n",
    "\n",
    "# Add color codes to the FI Risk column\n",
    "final_results_df[\"FI Risk\"] = final_results_df[\"FI Risk\"].apply(lambda x: color_mapping.get(x, \"\"))\n",
    "\n",
    "# Print the results with color-coded FI Risk column\n",
    "display(final_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763be608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "final_results_df.to_csv('[SAMPLE] [SVM] TF-IDF TextBlob (70-30) final new_data_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2499db",
   "metadata": {},
   "source": [
    "## 5. Visualization<a name=\"Section5\"></a>\n",
    "<br></br>\n",
    "**Type of Visualizations:**\n",
    "1. Heatmap\n",
    "2. Stacked Bar Chart\n",
    "Example: \n",
    "<img src=\"https://drive.google.com/file/d/1DPFEOBuwPYe4wVSVAvYoZYdVXlBMMTYh/view?usp=sharing\" alt=\"Example Stacked Bar Chart Model Evaluation Results\">\n",
    "3. ROC Curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6181db3",
   "metadata": {},
   "source": [
    "**Heatmap**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7131c1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Plotting the heatmap of confusion matrix\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b7d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Define the labels for the confusion matrix\n",
    "labels = ['True Negative', 'False Positive', 'False Negative', 'True Positive']\n",
    "\n",
    "# Create a new confusion matrix with the labels\n",
    "confusion_mat_labeled = np.empty((2,2), dtype=int)\n",
    "confusion_mat_labeled[0,0] = confusion_mat[0,0] # True Negative\n",
    "confusion_mat_labeled[0,1] = confusion_mat[0,1] # False Positive\n",
    "confusion_mat_labeled[1,0] = confusion_mat[1,0] # False Negative\n",
    "confusion_mat_labeled[1,1] = confusion_mat[1,1] # True Positive\n",
    "\n",
    "# Create a DataFrame with the confusion matrix and labels\n",
    "confusion_df = pd.DataFrame(confusion_mat_labeled, index=labels[:2], columns=labels[2:])\n",
    "\n",
    "# Create a heatmap of the confusion matrix\n",
    "sns.heatmap(confusion_df, annot=True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82c5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Function for Confusion Matrix\n",
    "def plot_cm(cm, classes, title, normalized = False, cmap = plt.cm.Blues):\n",
    "\n",
    "  plt.imshow(cm, interpolation = \"nearest\", cmap = cmap)\n",
    "  plt.title(title, pad = 20)\n",
    "  plt.colorbar()\n",
    "  tick_marks = np.arange(len(classes))\n",
    "  plt.xticks(tick_marks, classes)\n",
    "  plt.yticks(tick_marks, classes)\n",
    "\n",
    "  if normalized:\n",
    "    cm = cm.astype('float') / cm.sum(axis = 1)[: np.newaxis]\n",
    "    print(\"Normalized Confusion Matrix\")\n",
    "  else:\n",
    "    print(\"Unnormalized Confusion Matrix\")\n",
    "  \n",
    "  threshold = cm.max() / 2\n",
    "  for i in range(cm.shape[0]):\n",
    "    for j in range(cm.shape[1]):\n",
    "      plt.text(j, i, cm[i, j], horizontalalignment = \"center\", color = \"white\" if cm[i, j] > threshold else \"black\")\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.xlabel(\"Predicted Label\", labelpad = 20)\n",
    "  plt.ylabel(\"Real Label\", labelpad = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0dce7f",
   "metadata": {},
   "source": [
    "**For Harmonized Sentiment Tweets Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8779db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Perform predictions on the test data\n",
    "test_predictions = svm_classifier.predict(test_features_vectorized)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "confusion_mat = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "# Create a list of sentiment labels\n",
    "sentiment_labels = ['Low Risk', 'Mild Risk', 'Mild Negative', 'Strongly Negative']\n",
    "\n",
    "# Create a heatmap of the confusion matrix\n",
    "sns.heatmap(confusion_mat, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=sentiment_labels, yticklabels=sentiment_labels)\n",
    "\n",
    "# Set the axis labels and title\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5091d1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Create a dictionary to map sentiment labels to risk labels\n",
    "label_mapping = {sentiment_labels[i]: risk_labels[i] for i in range(len(sentiment_labels))}\n",
    "\n",
    "# Apply reassignment to the predicted sentiment labels\n",
    "results_df['predicted_risk'] = results_df['predicted_sentiment'].map(label_mapping)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "display(results_df)\n",
    "\n",
    "# Create a list of risk labels\n",
    "risk_labels = ['Low Risk', 'Mild Risk', 'Moderate Risk', 'Severe Risk']\n",
    "\n",
    "# Generate the confusion matrix\n",
    "confusion_mat = confusion_matrix(results_df['actual_risk'], results_df['predicted_risk'])\n",
    "\n",
    "# Create a heatmap of the confusion matrix\n",
    "sns.heatmap(confusion_mat, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=risk_labels, yticklabels=risk_labels)\n",
    "\n",
    "# Set the axis labels and title\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b3ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_labels = ['Positive', 'Weakly Negative', 'Mild Negative', 'Strongly Negative']\n",
    "risk_labels = ['Low Risk', 'Mild Risk', 'Moderate Risk', 'Severe Risk']\n",
    "\n",
    "# Create a dictionary to map sentiment labels to risk labels\n",
    "label_mapping = {sentiment_labels[i]: risk_labels[i] for i in range(len(sentiment_labels))}\n",
    "\n",
    "# Apply reassignment to the predicted sentiment labels\n",
    "results_df['predicted_risk'] = results_df['predicted_sentiment'].map(label_mapping)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89abfd31",
   "metadata": {},
   "source": [
    "## 6. Apply the Best SA Model on Full Dataset<a name=\"Section17\"></a>\n",
    "Back to [Top Page](#TOC)\n",
    "\n",
    "Using the trained model classifier, we can predict the sentiment of new text data\n",
    "<br>\n",
    "\n",
    "Positive [1] - Food Secured\n",
    "\n",
    "Negative [0] - Food Insecure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1804510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d1a05bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datetime</th>\n",
       "      <th>Username</th>\n",
       "      <th>Cleaned_Tweets</th>\n",
       "      <th>Location</th>\n",
       "      <th>VADER_score</th>\n",
       "      <th>TextBlob_score</th>\n",
       "      <th>Harmonized_Score</th>\n",
       "      <th>Harmonized_Label</th>\n",
       "      <th>Risk_Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27/1/2023 14:32</td>\n",
       "      <td>Don Dale</td>\n",
       "      <td>buying forget review first guy feel want comme...</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>0.6703</td>\n",
       "      <td>-0.250000</td>\n",
       "      <td>0.210150</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Low Risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27/1/2023 19:04</td>\n",
       "      <td>Iliani</td>\n",
       "      <td>food security research going explode issue end...</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>0.5859</td>\n",
       "      <td>-0.181818</td>\n",
       "      <td>0.202041</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Low Risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29/1/2023 8:28</td>\n",
       "      <td>Naim Zaini</td>\n",
       "      <td>context slaughtered food muslim consideration ...</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>0.8658</td>\n",
       "      <td>0.034722</td>\n",
       "      <td>0.450261</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Low Risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29/1/2023 13:29</td>\n",
       "      <td>??</td>\n",
       "      <td>raise food price wet good expensive sorry guy</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>0.3818</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.140900</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Low Risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30/1/2023 21:52</td>\n",
       "      <td>Alinosourawr</td>\n",
       "      <td>che restaurant sek send food x order food drin...</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>-0.8934</td>\n",
       "      <td>-0.433333</td>\n",
       "      <td>-0.663367</td>\n",
       "      <td>Strongly Negative</td>\n",
       "      <td>Severe Risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21832</th>\n",
       "      <td>2023-03-30 23:45:13+00:00</td>\n",
       "      <td>Charrlygirl</td>\n",
       "      <td>worried prosecution team family also worried f...</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>-0.8360</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.418000</td>\n",
       "      <td>Mild Negative</td>\n",
       "      <td>Moderate Risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21833</th>\n",
       "      <td>2023-03-30 23:49:23+00:00</td>\n",
       "      <td>angel19971102</td>\n",
       "      <td>love much clark must always worried bruce drea...</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>0.6939</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.546950</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Low Risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21834</th>\n",
       "      <td>2023-03-30 23:55:01+00:00</td>\n",
       "      <td>firdyfire</td>\n",
       "      <td>industry player worried energy commission chie...</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>-0.0258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.012900</td>\n",
       "      <td>Weakly Negative</td>\n",
       "      <td>Mild Risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21835</th>\n",
       "      <td>2023-03-30 23:55:16+00:00</td>\n",
       "      <td>AhmadMuhyie</td>\n",
       "      <td>ah really weak faith fasting without real exam...</td>\n",
       "      <td>Malaysia</td>\n",
       "      <td>0.6222</td>\n",
       "      <td>0.239583</td>\n",
       "      <td>0.430892</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Low Risk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21836</th>\n",
       "      <td>2023-03-30 23:57:11+00:00</td>\n",
       "      <td>farhinsyirh</td>\n",
       "      <td>hate sin man know recorded video amazes contin...</td>\n",
       "      <td>Johore, Malaysia</td>\n",
       "      <td>-0.2167</td>\n",
       "      <td>-0.288889</td>\n",
       "      <td>-0.252794</td>\n",
       "      <td>Weakly Negative</td>\n",
       "      <td>Mild Risk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21837 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Datetime       Username   \n",
       "0                27/1/2023 14:32       Don Dale  \\\n",
       "1                27/1/2023 19:04         Iliani   \n",
       "2                 29/1/2023 8:28     Naim Zaini   \n",
       "3                29/1/2023 13:29             ??   \n",
       "4                30/1/2023 21:52   Alinosourawr   \n",
       "...                          ...            ...   \n",
       "21832  2023-03-30 23:45:13+00:00    Charrlygirl   \n",
       "21833  2023-03-30 23:49:23+00:00  angel19971102   \n",
       "21834  2023-03-30 23:55:01+00:00      firdyfire   \n",
       "21835  2023-03-30 23:55:16+00:00    AhmadMuhyie   \n",
       "21836  2023-03-30 23:57:11+00:00    farhinsyirh   \n",
       "\n",
       "                                          Cleaned_Tweets          Location   \n",
       "0      buying forget review first guy feel want comme...          Malaysia  \\\n",
       "1      food security research going explode issue end...          Malaysia   \n",
       "2      context slaughtered food muslim consideration ...          Malaysia   \n",
       "3          raise food price wet good expensive sorry guy          Malaysia   \n",
       "4      che restaurant sek send food x order food drin...          Malaysia   \n",
       "...                                                  ...               ...   \n",
       "21832  worried prosecution team family also worried f...          Malaysia   \n",
       "21833  love much clark must always worried bruce drea...          Malaysia   \n",
       "21834  industry player worried energy commission chie...          Malaysia   \n",
       "21835  ah really weak faith fasting without real exam...          Malaysia   \n",
       "21836  hate sin man know recorded video amazes contin...  Johore, Malaysia   \n",
       "\n",
       "       VADER_score  TextBlob_score  Harmonized_Score   Harmonized_Label   \n",
       "0           0.6703       -0.250000          0.210150           Positive  \\\n",
       "1           0.5859       -0.181818          0.202041           Positive   \n",
       "2           0.8658        0.034722          0.450261           Positive   \n",
       "3           0.3818       -0.100000          0.140900           Positive   \n",
       "4          -0.8934       -0.433333         -0.663367  Strongly Negative   \n",
       "...            ...             ...               ...                ...   \n",
       "21832      -0.8360        0.000000         -0.418000      Mild Negative   \n",
       "21833       0.6939        0.400000          0.546950           Positive   \n",
       "21834      -0.0258        0.000000         -0.012900    Weakly Negative   \n",
       "21835       0.6222        0.239583          0.430892           Positive   \n",
       "21836      -0.2167       -0.288889         -0.252794    Weakly Negative   \n",
       "\n",
       "          Risk_Label  \n",
       "0           Low Risk  \n",
       "1           Low Risk  \n",
       "2           Low Risk  \n",
       "3           Low Risk  \n",
       "4        Severe Risk  \n",
       "...              ...  \n",
       "21832  Moderate Risk  \n",
       "21833       Low Risk  \n",
       "21834      Mild Risk  \n",
       "21835       Low Risk  \n",
       "21836      Mild Risk  \n",
       "\n",
       "[21837 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load csv file containing tweets dataset (w/ sentiments)\n",
    "\n",
    "tweets_df = pd.read_csv(r\"C:\\Users\\LENOVO\\Documents\\Degree Life\\FYP Journey\\Dataset\\Sentiment Analysis\\V3 Harmonized [VADER & TextBlob]_All Keywords (Whole Malaysia) - Copy.csv\")\n",
    "display(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad89f6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['buying forget review first guy feel want comment buying know fake food expensive'\n",
      " 'food security research going explode issue end example new rafizi docking expensive food one food security issue'\n",
      " 'context slaughtered food muslim consideration price cheap chicken bought heart sure halal level willing buy little chicken long feel confident observation chatting old people'\n",
      " ... 'industry player worried energy commission chief job'\n",
      " 'ah really weak faith fasting without real exam g angu know cm holding hungry thirst delayed bed easy see people eating really good patience sincerity practiced cm sweetener jargon status whatsapp'\n",
      " 'hate sin man know recorded video amazes continues prayer wrong despite tht easy feat yk faith come sacrifice eg org freehair hungry people fasting']\n"
     ]
    }
   ],
   "source": [
    "# Extract the features\n",
    "features_full = tweets_df['Cleaned_Tweets'].values\n",
    "print(features_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ffa2338",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Replace NaN values with an empty string in the feature set\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m features_full \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mwhere(pd\u001b[38;5;241m.\u001b[39misnull(features_full), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, features_full)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Transform the full dataset features using the trained vectorizer\u001b[39;00m\n\u001b[0;32m      5\u001b[0m features_full_vectorized \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mtransform(features_full)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Replace NaN values with an empty string in the feature set\n",
    "features_full = np.where(pd.isnull(features_full), '', features_full)\n",
    "\n",
    "# Transform the full dataset features using the trained vectorizer\n",
    "features_full_vectorized = vectorizer.transform(features_full)\n",
    "\n",
    "# Predict the sentiment labels using the trained model\n",
    "predicted_labels_full = svm_classifier.predict(features_full_vectorized)\n",
    "\n",
    "# Convert the numeric labels back to sentiment labels\n",
    "predicted_sentiments_full = encoder.inverse_transform(predicted_labels_full)\n",
    "\n",
    "# Map the sentiment labels to the corresponding risk labels\n",
    "predicted_risks_full = [label_mapping[label] for label in predicted_sentiments_full]\n",
    "\n",
    "# Add the predicted labels to the full dataset\n",
    "tweets_df['predicted_sentiment'] = predicted_sentiments_full\n",
    "tweets_df['predicted_risk'] = predicted_risks_full\n",
    "\n",
    "# Print the DataFrame\n",
    "display(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7afb448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the sentiment confusion matrix\n",
    "sentiment_confusion_mat = confusion_matrix(tweets_df['Harmonized_Label'], tweets_df['predicted_sentiment'])\n",
    "\n",
    "# Generate the risk confusion matrix\n",
    "risk_confusion_mat = confusion_matrix(tweets_df['Risk_Label'], tweets_df['predicted_risk'])\n",
    "\n",
    "# Create a list of sentiment labels\n",
    "sentiment_labels = ['Positive', 'Weakly Negative', 'Mild Negative', 'Strongly Negative']\n",
    "\n",
    "# Create a list of risk labels\n",
    "risk_labels = ['Low Risk', 'Mild Risk', 'Moderate Risk', 'Severe Risk']\n",
    "\n",
    "# Create a heatmap of the sentiment confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(sentiment_confusion_mat, annot=True, fmt='d', cmap='Blues', xticklabels=sentiment_labels, yticklabels=sentiment_labels)\n",
    "plt.title('Sentiment Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fca407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a heatmap of the risk confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(risk_confusion_mat, annot=True, fmt='d', cmap='Blues', xticklabels=risk_labels, yticklabels=risk_labels)\n",
    "plt.title('FI Risk Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
